{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The idea behind LDA is simple. Mathematically speaking, we need to find a new feature space to project the data in order to maximize classes separability. Well, obviously the first step is to find a way to measure this capacity of separation of each new feature space candidate. The distance between the projected means of each class could be one of the measures, however only this distance would not be a very good metric because it does not take the spread of data into account. I\n",
    "\n",
    "## Thus, we come up with two measures: the within-class and the between-class. However, this formulation is only possible if we assume that the dataset has a Normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ldaintro.jpg\" height=300 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Discriminant Analysis (LDA) can be used as a technique for feature extraction to increase the computational efficiency and reduce the degree of  over-fitting due to the curse of dimensionality in nonregularized models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA vs PCA :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/lda.png\" height=350 width=300 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general concept behind LDA is very similar to PCA:\n",
    "\n",
    "### **-  Whereas PCA attempts to find the orthogonal component axes of maximum variance in a dataset; the goal in LDA is to find the feature subspace that optimizes class separability.**\n",
    "\n",
    "### **-  Both LDA and PCA are linear transformation techniques that can be used to reduce the number of dimensions in a dataset; the former is an unsupervised algorithm, whereas the latter is supervised. **\n",
    "\n",
    "## Thus, we might intuitively think that LDA is a superior feature extraction technique for classification tasks compared to PCA. However, A.M. Martinez reported that preprocessing via PCA tends to result in better classification results in an image recognition task in certain cases, for instance, if each class consists of only a small number of samples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lda2.jpg\" height=300 width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining from the figure below:\n",
    "\n",
    "**A linear discriminant, as shown on the x-axis (LD 1), would separate the two normally distributed classes well.**\n",
    "\n",
    "-  Although the exemplary linear discriminant shown on the y-axis (LD 2) captures a lot of the variance in the dataset, it  would fail as a good linear discriminant since it does not capture any of the  class-discriminatory information. One assumption in LDA is that the data is normally distributed. \n",
    "\n",
    "-  Also, we assume that the classes have identical covariance matrices and that the features are statistically independent of each other. However, even if one or more of those assumptions are slightly violated, LDA for dimensionality reduction can still work reasonably well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lda3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section lists some suggestions you may consider when preparing your data for use with LDA.\n",
    "\n",
    "-  **Classification Problems**. This might go without saying, but LDA is intended for classification problems where the output variable is categorical. LDA supports both binary and multi-class classification.\n",
    "-  **Gaussian Distribution**. The standard implementation of the model assumes a Gaussian distribution of the input variables. Consider reviewing the univariate distributions of each attribute and using transforms to make them more Gaussian-looking (e.g. log and root for exponential distributions and Box-Cox for skewed distributions).\n",
    "-  **Remove Outliers**. Consider removing outliers from your data. These can skew the basic statistics used to separate classes in LDA such the mean and the standard deviation.\n",
    "-  **Same Variance**. LDA assumes that each input variable has the same variance. It is almost always a good idea to standardize your data before using LDA so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "_Below is an implementation on LDA on four featured IRIS Dataset_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lda4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine=pd.read_csv(\"dataset/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280  Proline  Customer_Segment  \n",
       "0   3.92     1065                 1  \n",
       "1   3.40     1050                 1  \n",
       "2   3.17     1185                 1  \n",
       "3   3.45     1480                 1  \n",
       "4   2.93      735                 1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we see above, there are many features, which make the measure of Feature EXtraction necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=wine.iloc[:,0:13]\n",
    "Y=wine.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape     # Thus the features are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178L,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape     # Thus the responses are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=2,test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45L,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since both LDA and PCA scale for the subset of data, we need to scale the data before applying the feature extraction technique. \n",
    "\n",
    "# The only difference between LDA and PCA is that while PCA has a dependant variable that is not taken as class, in LDA, this DV becomes classified as target. SO, LDA can be termed as a Supervised Extraction whereas PCA can be termed as Unsupervised Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "x_train=sc.fit_transform(X_train)\n",
    "x_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133L, 13L)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda=LinearDiscriminantAnalysis(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=lda.fit_transform(x_train,Y_train)\n",
    "x_test=lda.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45L, 2L)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(random_state=1)\n",
    "lr.fit(x_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=lr.predict(x_test)\n",
    "y_pred_train=lr.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0, 13,  1],\n",
       "       [ 0,  0, 12]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, as we see, In the model, for Class 1 no entry was misclassified. In Class 2, only one entry was falsely classified, and lastly, for Class 3 , no misentry took place, Thus implying high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40,  0,  0],\n",
       "       [ 0, 58,  0],\n",
       "       [ 0,  0, 35]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we see above, No entry of any Class was misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, we see the visualization of the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGRkBKCiIgASggiIhL6L0rJ+bruAyUnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6503c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the Test set results\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = x_test, Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, lr.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thus, this is how LDA takes place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
